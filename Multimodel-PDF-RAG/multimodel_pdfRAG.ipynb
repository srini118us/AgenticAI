{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2803bd",
   "metadata": {},
   "source": [
    "## MultiModal PDF-RAG with Langchain and OpenSearch Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d914d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Imports and Setup\n",
    "# =========================\n",
    "import os\n",
    "import time\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from docx import Document\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f50c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Load Environment Variables\n",
    "# =========================\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENSEARCH_HOST = os.getenv(\"OPENSEARCH_HOST\", \"localhost\")\n",
    "OPENSEARCH_PORT = int(os.getenv(\"OPENSEARCH_PORT\", 9200))\n",
    "OPENSEARCH_INDEX = os.getenv(\"OPENSEARCH_INDEX\", \"pdf_chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefce6ca",
   "metadata": {},
   "source": [
    "PDF Data Extraction:\n",
    "Extracts raw textual content from a PDF file. This forms the base input for downstream processing in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84bbd08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Step 1: Data Ingestion from PDF using PyMuPDFLoader\n",
    "# =========================\n",
    "def extract_text_and_charts(pdf_path):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    print(f\"âœ… Extracted {len(texts)} pages from PDF\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736719ef",
   "metadata": {},
   "source": [
    "Semantic Chunking :\n",
    "Splits the extracted content into semantically meaningful text chunks. This ensures better context preservation for both vectorization and LLM processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8b5c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Step 2: Semantic Chunking\n",
    "# =========================\n",
    "def semantic_chunking(texts, chunk_size=800, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"]\n",
    "    )\n",
    "    chunks = []\n",
    "    for t in texts:\n",
    "        chunks.extend(splitter.split_text(t))\n",
    "    print(f\"âœ… Generated {len(chunks)} semantic chunks\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f016b2",
   "metadata": {},
   "source": [
    "Embedding and Indexing\n",
    "Converts text chunks into vector embeddings using an OpenAI model and stores them in an OpenSearch vector index. This enables fast and scalable semantic search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00ff26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Step 3: Embedding and Storing in OpenSearch\n",
    "# =========================\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "def embed_and_store_chunks_opensearch(chunks):\n",
    "    client = OpenSearch(\n",
    "        hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "        http_compress=True\n",
    "    )\n",
    "    vectorstore = OpenSearchVectorSearch(\n",
    "    index_name=OPENSEARCH_INDEX,\n",
    "    embedding_function=embedding_model,  # Pass the full embedding model\n",
    "    opensearch_url=f\"http://{OPENSEARCH_HOST}:{OPENSEARCH_PORT}\",\n",
    "    http_auth=None,\n",
    "    use_ssl=False,\n",
    "    )\n",
    "    vectorstore.add_texts(chunks)\n",
    "    print(f\"âœ… Embedded and stored {len(chunks)} chunks in OpenSearch\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81b8fe",
   "metadata": {},
   "source": [
    "Semantic Search:\n",
    "Uses a natural language query to search the vector index and retrieve the most relevant text chunks based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb801fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Step 4: Semantic Search with OpenSearch\n",
    "# =========================\n",
    "def search_opensearch(vectorstore, query, top_k=5):\n",
    "    start = time.time()\n",
    "    results = vectorstore.similarity_search(query, k=top_k)\n",
    "    duration = time.time() - start\n",
    "    print(f\"âœ… Search completed in {duration:.2f} seconds, top {top_k} results retrieved\")\n",
    "    return results, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ebaee9",
   "metadata": {},
   "source": [
    "LLM Response Generation\n",
    "Constructs a structured prompt using the retrieved context and query, and generates a coherent, well-formed response using a GPT-4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2ed662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Step 5: Prompt Template and LLM Output Generation\n",
    "# =========================\n",
    "def generate_response(context, query):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=(\n",
    "            \"You are an AI assistant helping to summarize and explain research papers.\\n\"\n",
    "            \"Based on the following context extracted from the paper 'Attention Is All You Need':\\n\\n\"\n",
    "            \"{context}\\n\\n\"\n",
    "            \"Please provide a clear and concise answer to the following question:\\n\"\n",
    "            \"{query}\\n\"\n",
    "            \"Your response should be accurate, well-structured, and use appropriate technical terminology.\"\n",
    "        )\n",
    "    )\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.2)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = chain.run({\"context\": context, \"query\": query})\n",
    "    print(\"âœ… LLM response generated\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408f144",
   "metadata": {},
   "source": [
    "Save Output:\n",
    "Saves the generated response into a DOCX document, allowing users to archive or share results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b01df8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Step 6: Save LLM Response to DOCX\n",
    "# =========================\n",
    "def save_response_to_docx(response_text, filename=\"response.docx\"):\n",
    "    doc = Document()\n",
    "    doc.add_paragraph(response_text)\n",
    "    doc.save(filename)\n",
    "    print(f\"âœ… Response saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df51e21",
   "metadata": {},
   "source": [
    "Full Pipeline Orchestration:\n",
    "Chains all the above steps into a seamless end-to-end flow â€” from PDF ingestion to LLM-powered answer generation and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a2ba617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Step 7: Full Pipeline Execution\n",
    "# =========================\n",
    "def run_pipeline(pdf_path, user_query):\n",
    "    print(\"ðŸ”¹ Extracting data from PDF...\")\n",
    "    combined_texts = extract_text_and_charts(pdf_path)\n",
    "\n",
    "    print(\"ðŸ”¹ Performing semantic chunking...\")\n",
    "    chunks = semantic_chunking(combined_texts)\n",
    "\n",
    "    print(\"ðŸ”¹ Embedding chunks and storing in OpenSearch...\")\n",
    "    vectorstore = embed_and_store_chunks_opensearch(chunks)\n",
    "\n",
    "    print(\"ðŸ”¹ Performing similarity search...\")\n",
    "    results, _ = search_opensearch(vectorstore, user_query)\n",
    "\n",
    "    top_context = \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    print(\"ðŸ”¹ Generating response using LLM...\")\n",
    "    response = generate_response(top_context, user_query)\n",
    "\n",
    "    save_response_to_docx(response)\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc47921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Extracting data from PDF...\n",
      "âœ… Extracted 11 pages from PDF\n",
      "ðŸ”¹ Performing semantic chunking...\n",
      "âœ… Generated 51 semantic chunks\n",
      "ðŸ”¹ Embedding chunks and storing in OpenSearch...\n",
      "âœ… Embedded and stored 51 chunks in OpenSearch\n",
      "ðŸ”¹ Performing similarity search...\n",
      "âœ… Search completed in 0.33 seconds, top 5 results retrieved\n",
      "ðŸ”¹ Generating response using LLM...\n",
      "âœ… LLM response generated\n",
      "âœ… Response saved to response.docx\n",
      "The core idea behind the self-attention mechanism, as proposed in the paper \"Attention Is All You Need,\" is to compute a representation of a sequence by relating different positions within that sequence. Self-attention, also known as intra-attention, allows the model to weigh the importance of each position in the sequence relative to others, enabling it to capture dependencies between distant positions efficiently.\n",
      "\n",
      "In traditional sequence models, capturing long-range dependencies can be computationally expensive and challenging. However, self-attention reduces this complexity to a constant number of operations, allowing the model to focus on relevant parts of the sequence regardless of their distance from each other. This is achieved by calculating attention scores for each position, which are then used to create a weighted sum of the input features, effectively highlighting important parts of the sequence.\n",
      "\n",
      "The paper addresses a potential downside of self-attention, which is the reduced effective resolution due to averaging attention-weighted positions. This issue is mitigated through the use of Multi-Head Attention, which allows the model to attend to information from different representation subspaces at different positions, thereby enhancing its ability to capture complex patterns and relationships within the data.\n",
      "\n",
      "Self-attention has been successfully applied to various tasks, including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations, demonstrating its versatility and effectiveness in processing sequential data.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ðŸ”¹ Run Example\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\n",
    "        pdf_path=\"attention.pdf\",\n",
    "        user_query=\"Can you explain the core idea behind the self-attention mechanism proposed in the paper?\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
